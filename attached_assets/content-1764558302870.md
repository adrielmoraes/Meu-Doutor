[Ir para o conteúdo principal](https://ai.google.dev/api/tokens?hl=pt-br#main-content)

[![Gemini API](https://ai.google.dev/_static/googledevai/images/gemini-api-logo.svg?hl=pt-br)](https://ai.google.dev/)

`/`

Language

- [English](https://ai.google.dev/api/tokens)
- [Deutsch](https://ai.google.dev/api/tokens?hl=de)
- [Español – América Latina](https://ai.google.dev/api/tokens?hl=es-419)
- [Français](https://ai.google.dev/api/tokens?hl=fr)
- [Indonesia](https://ai.google.dev/api/tokens?hl=id)
- [Italiano](https://ai.google.dev/api/tokens?hl=it)
- [Polski](https://ai.google.dev/api/tokens?hl=pl)
- [Português – Brasil](https://ai.google.dev/api/tokens?hl=pt-br)
- [Shqip](https://ai.google.dev/api/tokens?hl=sq)
- [Tiếng Việt](https://ai.google.dev/api/tokens?hl=vi)
- [Türkçe](https://ai.google.dev/api/tokens?hl=tr)
- [Русский](https://ai.google.dev/api/tokens?hl=ru)
- [עברית](https://ai.google.dev/api/tokens?hl=he)
- [العربيّة](https://ai.google.dev/api/tokens?hl=ar)
- [فارسی](https://ai.google.dev/api/tokens?hl=fa)
- [हिंदी](https://ai.google.dev/api/tokens?hl=hi)
- [বাংলা](https://ai.google.dev/api/tokens?hl=bn)
- [ภาษาไทย](https://ai.google.dev/api/tokens?hl=th)
- [中文 – 简体](https://ai.google.dev/api/tokens?hl=zh-cn)
- [中文 – 繁體](https://ai.google.dev/api/tokens?hl=zh-tw)
- [日本語](https://ai.google.dev/api/tokens?hl=ja)
- [한국어](https://ai.google.dev/api/tokens?hl=ko)

[Conferir a chave de API](https://aistudio.google.com/apikey?hl=pt-br) [Manual](https://github.com/google-gemini/cookbook) [Comunidade](https://discuss.ai.google.dev/c/gemini-api/?hl=pt-br)

[Fazer login](https://ai.google.dev/_d/signin?continue=https%3A%2F%2Fai.google.dev%2Fapi%2Ftokens%3Fhl%3Dpt-br&prompt=select_account)

- Nesta página
- [Método: models.countTokens](https://ai.google.dev/api/tokens?hl=pt-br#method:-models.counttokens)
  - [Endpoint](https://ai.google.dev/api/tokens?hl=pt-br#endpoint)
  - [Parâmetros de caminho](https://ai.google.dev/api/tokens?hl=pt-br#path-parameters)
  - [Corpo da solicitação](https://ai.google.dev/api/tokens?hl=pt-br#request-body)
  - [Exemplo de solicitação](https://ai.google.dev/api/tokens?hl=pt-br#example-request)
  - [Corpo da resposta](https://ai.google.dev/api/tokens?hl=pt-br#response-body)


O Gemini 3 Pro já está disponível. [Teste sem custo financeiro no Google AI Studio](https://aistudio.google.com/?model=gemini-3-pro-preview&hl=pt-br).




![](https://ai.google.dev/_static/images/translated.svg?hl=pt-br)

Esta página foi traduzida pela [API Cloud Translation](https://cloud.google.com/translate/?hl=pt-br).


Switch to English


- [Página inicial](https://ai.google.dev/?hl=pt-br)
- [Gemini API](https://ai.google.dev/gemini-api?hl=pt-br)
- [Referência da API](https://ai.google.dev/api?hl=pt-br)

Isso foi útil?



 Envie comentários



# Counting tokens    content\_copy

- Nesta página
- [Método: models.countTokens](https://ai.google.dev/api/tokens?hl=pt-br#method:-models.counttokens)
  - [Endpoint](https://ai.google.dev/api/tokens?hl=pt-br#endpoint)
  - [Parâmetros de caminho](https://ai.google.dev/api/tokens?hl=pt-br#path-parameters)
  - [Corpo da solicitação](https://ai.google.dev/api/tokens?hl=pt-br#request-body)
  - [Exemplo de solicitação](https://ai.google.dev/api/tokens?hl=pt-br#example-request)
  - [Corpo da resposta](https://ai.google.dev/api/tokens?hl=pt-br#response-body)

Para um guia detalhado sobre como contar tokens usando a API Gemini, incluindo como imagens, áudio e vídeo são contados, consulte o [Guia de contagem de tokens](https://ai.google.dev/gemini-api/docs/tokens?hl=pt-br) e a [Receita do livro de receitas](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Counting_Tokens.ipynb?hl=pt-br).

## Método: models.countTokens

- [Endpoint](https://ai.google.dev/api/tokens?hl=pt-br#body.HTTP_TEMPLATE)
- [Parâmetros de caminho](https://ai.google.dev/api/tokens?hl=pt-br#body.PATH_PARAMETERS)
- [Corpo da solicitação](https://ai.google.dev/api/tokens?hl=pt-br#body.request_body)
  - [Representação JSON](https://ai.google.dev/api/tokens?hl=pt-br#body.request_body.SCHEMA_REPRESENTATION)
- [Corpo da resposta](https://ai.google.dev/api/tokens?hl=pt-br#body.response_body)
  - [Representação JSON](https://ai.google.dev/api/tokens?hl=pt-br#body.CountTokensResponse.SCHEMA_REPRESENTATION)
- [Escopos de autorização](https://ai.google.dev/api/tokens?hl=pt-br#body.aspect)
- [Exemplo de solicitação](https://ai.google.dev/api/tokens?hl=pt-br#body.codeSnippets)
  - [Texto](https://ai.google.dev/api/tokens?hl=pt-br#body.codeSnippets.group)
  - [Chat](https://ai.google.dev/api/tokens?hl=pt-br#body.codeSnippets.group_1)
  - [Mídia inline](https://ai.google.dev/api/tokens?hl=pt-br#body.codeSnippets.group_2)
  - [Vídeo](https://ai.google.dev/api/tokens?hl=pt-br#body.codeSnippets.group_3)
  - [PDF](https://ai.google.dev/api/tokens?hl=pt-br#body.codeSnippets.group_4)
  - [Cache](https://ai.google.dev/api/tokens?hl=pt-br#body.codeSnippets.group_5)
  - [Instrução do sistema](https://ai.google.dev/api/tokens?hl=pt-br#body.codeSnippets.group_6)
  - [Ferramentas](https://ai.google.dev/api/tokens?hl=pt-br#body.codeSnippets.group_7)

Executa o tokenizador de um modelo na entrada `Content` e retorna a contagem de tokens. Consulte o [guia de tokens](https://ai.google.dev/gemini-api/docs/tokens?hl=pt-br) para saber mais.

### Endpoint

post
`https://generativelanguage.googleapis.com/v1beta/{model=models/*}:countTokens`

### Parâmetros de caminho

`model``string`

Obrigatório. O nome do recurso do modelo. Ele serve como um ID para o modelo usar.

Esse nome precisa corresponder a um nome de modelo retornado pelo método `models.list`.

Formato: `models/{model}`. Ele tem o formato `models/{model}`.

### Corpo da solicitação

O corpo da solicitação contém dados com a seguinte estrutura:

Campos

`contents[]``object (Content)`

Opcional. A entrada fornecida ao modelo como um comando. Esse campo é ignorado quando `generateContentRequest` é definido.

`generateContentRequest``object (GenerateContentRequest)`

Opcional. A entrada geral fornecida ao `Model`. Isso inclui o comando e outras informações de direcionamento do modelo, como [instruções do sistema](https://ai.google.dev/gemini-api/docs/system-instructions?hl=pt-br) e/ou declarações de função para [chamada de função](https://ai.google.dev/gemini-api/docs/function-calling?hl=pt-br). `Model`s/`Content`s e `generateContentRequest`s são mutuamente exclusivos. Você pode enviar `Model` \+ `Content`s ou um `generateContentRequest`, mas nunca ambos.

### Exemplo de solicitação

[Texto](https://ai.google.dev/api/tokens?hl=pt-br#texto)[Chat](https://ai.google.dev/api/tokens?hl=pt-br#chat)[Mídia inline](https://ai.google.dev/api/tokens?hl=pt-br#m%C3%ADdia-inline)[Vídeo](https://ai.google.dev/api/tokens?hl=pt-br#v%C3%ADdeo)[PDF](https://ai.google.dev/api/tokens?hl=pt-br#pdf)[Cache](https://ai.google.dev/api/tokens?hl=pt-br#cache)[Instrução do sistema](https://ai.google.dev/api/tokens?hl=pt-br#instru%C3%A7%C3%A3o-do-sistema)[Ferramentas](https://ai.google.dev/api/tokens?hl=pt-br#ferramentas)Mais

[Python](https://ai.google.dev/api/tokens?hl=pt-br#python)[Node.js](https://ai.google.dev/api/tokens?hl=pt-br#node.js)[Go](https://ai.google.dev/api/tokens?hl=pt-br#go)[Concha](https://ai.google.dev/api/tokens?hl=pt-br#concha)Mais

```
from google import genai

client = genai.Client()
prompt = "The quick brown fox jumps over the lazy dog."

# Count tokens using the new client method.
total_tokens = client.models.count_tokens(
    model="gemini-2.0-flash", contents=prompt
)
print("total_tokens: ", total_tokens)
# ( e.g., total_tokens: 10 )

response = client.models.generate_content(
    model="gemini-2.0-flash", contents=prompt
)

# The usage_metadata provides detailed token counts.
print(response.usage_metadata)
# ( e.g., prompt_token_count: 11, candidates_token_count: 73, total_token_count: 84 )count_tokens.py
```

```
// Make sure to include the following import:
// import {GoogleGenAI} from '@google/genai';
const ai = new GoogleGenAI({ apiKey: process.env.GEMINI_API_KEY });
const prompt = "The quick brown fox jumps over the lazy dog.";
const countTokensResponse = await ai.models.countTokens({
  model: "gemini-2.0-flash",
  contents: prompt,
});
console.log(countTokensResponse.totalTokens);

const generateResponse = await ai.models.generateContent({
  model: "gemini-2.0-flash",
  contents: prompt,
});
console.log(generateResponse.usageMetadata);count_tokens.js
```

```
ctx := context.Background()
client, err := genai.NewClient(ctx, &genai.ClientConfig{
	APIKey:  os.Getenv("GEMINI_API_KEY"),
	Backend: genai.BackendGeminiAPI,
})
if err != nil {
	log.Fatal(err)
}
prompt := "The quick brown fox jumps over the lazy dog."

// Convert prompt to a slice of *genai.Content using the helper.
contents := []*genai.Content{
	genai.NewContentFromText(prompt, genai.RoleUser),
}
countResp, err := client.Models.CountTokens(ctx, "gemini-2.0-flash", contents, nil)
if err != nil {
	return err
}
fmt.Println("total_tokens:", countResp.TotalTokens)

response, err := client.Models.GenerateContent(ctx, "gemini-2.0-flash", contents, nil)
if err != nil {
	log.Fatal(err)
}
usageMetadata, err := json.MarshalIndent(response.UsageMetadata, "", "  ")
if err != nil {
	log.Fatal(err)
}
fmt.Println(string(usageMetadata))count_tokens.go
```

```
curl https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:countTokens?key=$GEMINI_API_KEY \
    -H 'Content-Type: application/json' \
    -X POST \
    -d '{
      "contents": [{\
        "parts":[{\
          "text": "The quick brown fox jumps over the lazy dog."\
          }],\
        }],
      }'count_tokens.sh
```

[Python](https://ai.google.dev/api/tokens?hl=pt-br#python)[Node.js](https://ai.google.dev/api/tokens?hl=pt-br#node.js)[Go](https://ai.google.dev/api/tokens?hl=pt-br#go)[Concha](https://ai.google.dev/api/tokens?hl=pt-br#concha)Mais

```
from google import genai
from google.genai import types

client = genai.Client()

chat = client.chats.create(
    model="gemini-2.0-flash",
    history=[\
        types.Content(\
            role="user", parts=[types.Part(text="Hi my name is Bob")]\
        ),\
        types.Content(role="model", parts=[types.Part(text="Hi Bob!")]),\
    ],
)
# Count tokens for the chat history.
print(
    client.models.count_tokens(
        model="gemini-2.0-flash", contents=chat.get_history()
    )
)
# ( e.g., total_tokens: 10 )

response = chat.send_message(
    message="In one sentence, explain how a computer works to a young child."
)
print(response.usage_metadata)
# ( e.g., prompt_token_count: 25, candidates_token_count: 21, total_token_count: 46 )

# You can count tokens for the combined history and a new message.
extra = types.UserContent(
    parts=[\
        types.Part(\
            text="What is the meaning of life?",\
        )\
    ]
)
history = chat.get_history()
history.append(extra)
print(client.models.count_tokens(model="gemini-2.0-flash", contents=history))
# ( e.g., total_tokens: 56 )count_tokens.py
```

```
// Make sure to include the following import:
// import {GoogleGenAI} from '@google/genai';
const ai = new GoogleGenAI({ apiKey: process.env.GEMINI_API_KEY });
// Initial chat history.
const history = [\
  { role: "user", parts: [{ text: "Hi my name is Bob" }] },\
  { role: "model", parts: [{ text: "Hi Bob!" }] },\
];
const chat = ai.chats.create({
  model: "gemini-2.0-flash",
  history: history,
});

// Count tokens for the current chat history.
const countTokensResponse = await ai.models.countTokens({
  model: "gemini-2.0-flash",
  contents: chat.getHistory(),
});
console.log(countTokensResponse.totalTokens);

const chatResponse = await chat.sendMessage({
  message: "In one sentence, explain how a computer works to a young child.",
});
console.log(chatResponse.usageMetadata);

// Add an extra user message to the history.
const extraMessage = {
  role: "user",
  parts: [{ text: "What is the meaning of life?" }],
};
const combinedHistory = chat.getHistory();
combinedHistory.push(extraMessage);
const combinedCountTokensResponse = await ai.models.countTokens({
  model: "gemini-2.0-flash",
  contents: combinedHistory,
});
console.log(
  "Combined history token count:",
  combinedCountTokensResponse.totalTokens,
);count_tokens.js
```

```
ctx := context.Background()
client, err := genai.NewClient(ctx, &genai.ClientConfig{
	APIKey:  os.Getenv("GEMINI_API_KEY"),
	Backend: genai.BackendGeminiAPI,
})
if err != nil {
	log.Fatal(err)
}

// Initialize chat with some history.
history := []*genai.Content{
	{Role: genai.RoleUser, Parts: []*genai.Part{{Text: "Hi my name is Bob"}}},
	{Role: genai.RoleModel, Parts: []*genai.Part{{Text: "Hi Bob!"}}},
}
chat, err := client.Chats.Create(ctx, "gemini-2.0-flash", nil, history)
if err != nil {
	log.Fatal(err)
}

firstTokenResp, err := client.Models.CountTokens(ctx, "gemini-2.0-flash", chat.History(false), nil)
if err != nil {
	log.Fatal(err)
}
fmt.Println(firstTokenResp.TotalTokens)

resp, err := chat.SendMessage(ctx, genai.Part{
	Text: "In one sentence, explain how a computer works to a young child."},
)
if err != nil {
	log.Fatal(err)
}
fmt.Printf("%#v\n", resp.UsageMetadata)

// Append an extra user message and recount.
extra := genai.NewContentFromText("What is the meaning of life?", genai.RoleUser)
hist := chat.History(false)
hist = append(hist, extra)

secondTokenResp, err := client.Models.CountTokens(ctx, "gemini-2.0-flash", hist, nil)
if err != nil {
	log.Fatal(err)
}
fmt.Println(secondTokenResp.TotalTokens)count_tokens.go
```

```
curl https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:countTokens?key=$GEMINI_API_KEY \
    -H 'Content-Type: application/json' \
    -X POST \
    -d '{
      "contents": [\
        {"role": "user",\
        "parts": [{"text": "Hi, my name is Bob."}],\
        },\
        {"role": "model",\
         "parts":[{"text": "Hi Bob"}],\
        },\
      ],
      }'count_tokens.sh
```

[Python](https://ai.google.dev/api/tokens?hl=pt-br#python)[Node.js](https://ai.google.dev/api/tokens?hl=pt-br#node.js)[Go](https://ai.google.dev/api/tokens?hl=pt-br#go)[Concha](https://ai.google.dev/api/tokens?hl=pt-br#concha)Mais

```
from google import genai
import PIL.Image

client = genai.Client()
prompt = "Tell me about this image"
your_image_file = PIL.Image.open(media / "organ.jpg")

# Count tokens for combined text and inline image.
print(
    client.models.count_tokens(
        model="gemini-2.0-flash", contents=[prompt, your_image_file]
    )
)
# ( e.g., total_tokens: 263 )

response = client.models.generate_content(
    model="gemini-2.0-flash", contents=[prompt, your_image_file]
)
print(response.usage_metadata)
# ( e.g., prompt_token_count: 264, candidates_token_count: 80, total_token_count: 345 )count_tokens.py
```

```
// Make sure to include the following import:
// import {GoogleGenAI} from '@google/genai';
const ai = new GoogleGenAI({ apiKey: process.env.GEMINI_API_KEY });
const prompt = "Tell me about this image";
const imageBuffer = fs.readFileSync(path.join(media, "organ.jpg"));

// Convert buffer to base64 string.
const imageBase64 = imageBuffer.toString("base64");

// Build contents using createUserContent and createPartFromBase64.
const contents = createUserContent([\
  prompt,\
  createPartFromBase64(imageBase64, "image/jpeg"),\
]);

const countTokensResponse = await ai.models.countTokens({
  model: "gemini-2.0-flash",
  contents: contents,
});
console.log(countTokensResponse.totalTokens);

const generateResponse = await ai.models.generateContent({
  model: "gemini-2.0-flash",
  contents: contents,
});
console.log(generateResponse.usageMetadata);count_tokens.js
```

```
model := client.GenerativeModel("gemini-1.5-flash")
prompt := "Tell me about this image"
imageFile, err := os.ReadFile(filepath.Join(testDataDir, "personWorkingOnComputer.jpg"))
if err != nil {
	log.Fatal(err)
}
// Call `CountTokens` to get the input token count
// of the combined text and file (`total_tokens`).
// An image's display or file size does not affect its token count.
// Optionally, you can call `count_tokens` for the text and file separately.
tokResp, err := model.CountTokens(ctx, genai.Text(prompt), genai.ImageData("jpeg", imageFile))
if err != nil {
	log.Fatal(err)
}
fmt.Println("total_tokens:", tokResp.TotalTokens)
// ( total_tokens: 264 )

resp, err := model.GenerateContent(ctx, genai.Text(prompt), genai.ImageData("jpeg", imageFile))
if err != nil {
	log.Fatal(err)
}

fmt.Println("prompt_token_count:", resp.UsageMetadata.PromptTokenCount)
fmt.Println("candidates_token_count:", resp.UsageMetadata.CandidatesTokenCount)
fmt.Println("total_token_count:", resp.UsageMetadata.TotalTokenCount)
// ( prompt_token_count: 264, candidates_token_count: 100, total_token_count: 364 )docs-snippets_test.go
```

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:countTokens?key=$GEMINI_API_KEY" \
    -H 'Content-Type: application/json' \
    -X POST \
    -d '{
      "contents": [{\
        "parts":[\
            {"text": "Tell me about this instrument"},\
            {\
              "inline_data": {\
                "mime_type":"image/jpeg",\
                "data": "'$(base64 $B64FLAGS $IMG_PATH)'"\
              }\
            }\
        ]\
        }]
       }' 2> /dev/nullcount_tokens.sh
```

[Python](https://ai.google.dev/api/tokens?hl=pt-br#python)[Node.js](https://ai.google.dev/api/tokens?hl=pt-br#node.js)[Go](https://ai.google.dev/api/tokens?hl=pt-br#go)[Concha](https://ai.google.dev/api/tokens?hl=pt-br#concha)Mais

```
from google import genai
import time

client = genai.Client()
prompt = "Tell me about this video"
your_file = client.files.upload(file=media / "Big_Buck_Bunny.mp4")

# Poll until the video file is completely processed (state becomes ACTIVE).
while not your_file.state or your_file.state.name != "ACTIVE":
    print("Processing video...")
    print("File state:", your_file.state)
    time.sleep(5)
    your_file = client.files.get(name=your_file.name)

print(
    client.models.count_tokens(
        model="gemini-2.0-flash", contents=[prompt, your_file]
    )
)
# ( e.g., total_tokens: 300 )

response = client.models.generate_content(
    model="gemini-2.0-flash", contents=[prompt, your_file]
)
print(response.usage_metadata)
# ( e.g., prompt_token_count: 301, candidates_token_count: 60, total_token_count: 361 )count_tokens.py
```

```
// Make sure to include the following import:
// import {GoogleGenAI} from '@google/genai';
const ai = new GoogleGenAI({ apiKey: process.env.GEMINI_API_KEY });
const prompt = "Tell me about this video";
let videoFile = await ai.files.upload({
  file: path.join(media, "Big_Buck_Bunny.mp4"),
  config: { mimeType: "video/mp4" },
});

// Poll until the video file is completely processed (state becomes ACTIVE).
while (!videoFile.state || videoFile.state.toString() !== "ACTIVE") {
  console.log("Processing video...");
  console.log("File state: ", videoFile.state);
  await sleep(5000);
  videoFile = await ai.files.get({ name: videoFile.name });
}

const countTokensResponse = await ai.models.countTokens({
  model: "gemini-2.0-flash",
  contents: createUserContent([\
    prompt,\
    createPartFromUri(videoFile.uri, videoFile.mimeType),\
  ]),
});
console.log(countTokensResponse.totalTokens);

const generateResponse = await ai.models.generateContent({
  model: "gemini-2.0-flash",
  contents: createUserContent([\
    prompt,\
    createPartFromUri(videoFile.uri, videoFile.mimeType),\
  ]),
});
console.log(generateResponse.usageMetadata);count_tokens.js
```

```
ctx := context.Background()
client, err := genai.NewClient(ctx, &genai.ClientConfig{
	APIKey:  os.Getenv("GEMINI_API_KEY"),
	Backend: genai.BackendGeminiAPI,
})
if err != nil {
	log.Fatal(err)
}

file, err := client.Files.UploadFromPath(
	ctx,
	filepath.Join(getMedia(), "Big_Buck_Bunny.mp4"),
	&genai.UploadFileConfig{
		MIMEType : "video/mp4",
	},
)
if err != nil {
	log.Fatal(err)
}

// Poll until the video file is completely processed (state becomes ACTIVE).
for file.State == genai.FileStateUnspecified || file.State != genai.FileStateActive {
	fmt.Println("Processing video...")
	fmt.Println("File state:", file.State)
	time.Sleep(5 * time.Second)

	file, err = client.Files.Get(ctx, file.Name, nil)
	if err != nil {
		log.Fatal(err)
	}
}

parts := []*genai.Part{
	genai.NewPartFromText("Tell me about this video"),
	genai.NewPartFromURI(file.URI, file.MIMEType),
}
contents := []*genai.Content{
	genai.NewContentFromParts(parts, genai.RoleUser),
}

tokenResp, err := client.Models.CountTokens(ctx, "gemini-2.0-flash", contents, nil)
if err != nil {
	log.Fatal(err)
}
fmt.Println("Multimodal video/audio token count:", tokenResp.TotalTokens)
response, err := client.Models.GenerateContent(ctx, "gemini-2.0-flash", contents, nil)
if err != nil {
	log.Fatal(err)
}
usageMetadata, err := json.MarshalIndent(response.UsageMetadata, "", "  ")
if err != nil {
	log.Fatal(err)
}
fmt.Println(string(usageMetadata))count_tokens.go
```

```

MIME_TYPE=$(file -b --mime-type "${VIDEO_PATH}")
NUM_BYTES=$(wc -c < "${VIDEO_PATH}")
DISPLAY_NAME=VIDEO_PATH

# Initial resumable request defining metadata.
# The upload url is in the response headers dump them to a file.
curl "${BASE_URL}/upload/v1beta/files?key=${GOOGLE_API_KEY}" \
  -D upload-header.tmp \
  -H "X-Goog-Upload-Protocol: resumable" \
  -H "X-Goog-Upload-Command: start" \
  -H "X-Goog-Upload-Header-Content-Length: ${NUM_BYTES}" \
  -H "X-Goog-Upload-Header-Content-Type: ${MIME_TYPE}" \
  -H "Content-Type: application/json" \
  -d "{'file': {'display_name': '${DISPLAY_NAME}'}}" 2> /dev/null

upload_url=$(grep -i "x-goog-upload-url: " "${tmp_header_file}" | cut -d" " -f2 | tr -d "\r")
rm "${tmp_header_file}"

# Upload the actual bytes.
curl "${upload_url}" \
  -H "Content-Length: ${NUM_BYTES}" \
  -H "X-Goog-Upload-Offset: 0" \
  -H "X-Goog-Upload-Command: upload, finalize" \
  --data-binary "@${VIDEO_PATH}" 2> /dev/null > file_info.json

file_uri=$(jq ".file.uri" file_info.json)

state=$(jq ".file.state" file_info.json)

name=$(jq ".file.name" file_info.json)

while [[ "($state)" = *"PROCESSING"* ]];
do
  echo "Processing video..."
  sleep 5
  # Get the file of interest to check state
  curl https://generativelanguage.googleapis.com/v1beta/files/$name > file_info.json
  state=$(jq ".file.state" file_info.json)
done

curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:countTokens?key=$GOOGLE_API_KEY" \
    -H 'Content-Type: application/json' \
    -X POST \
    -d '{
      "contents": [{\
        "parts":[\
          {"text": "Describe this video clip"},\
          {"file_data":{"mime_type": "video/mp4", "file_uri": '$file_uri'}}]\
        }]
       }'count_tokens.sh
```

[Python](https://ai.google.dev/api/tokens?hl=pt-br#python)Mais

```
from google import genai

client = genai.Client()
sample_pdf = client.files.upload(file=media / "test.pdf")
token_count = client.models.count_tokens(
    model="gemini-2.0-flash",
    contents=["Give me a summary of this document.", sample_pdf],
)
print(f"{token_count=}")

response = client.models.generate_content(
    model="gemini-2.0-flash",
    contents=["Give me a summary of this document.", sample_pdf],
)
print(response.usage_metadata)count_tokens.py
```

[Python](https://ai.google.dev/api/tokens?hl=pt-br#python)[Node.js](https://ai.google.dev/api/tokens?hl=pt-br#node.js)[Go](https://ai.google.dev/api/tokens?hl=pt-br#go)Mais

```
from google import genai
from google.genai import types
import time

client = genai.Client()
your_file = client.files.upload(file=media / "a11.txt")

cache = client.caches.create(
    model="gemini-1.5-flash-001",
    config={
        "contents": ["Here the Apollo 11 transcript:", your_file],
        "system_instruction": None,
        "tools": None,
    },
)

# Create a prompt.
prompt = "Please give a short summary of this file."

# Count tokens for the prompt (the cached content is not passed here).
print(client.models.count_tokens(model="gemini-2.0-flash", contents=prompt))
# ( e.g., total_tokens: 9 )

response = client.models.generate_content(
    model="gemini-1.5-flash-001",
    contents=prompt,
    config=types.GenerateContentConfig(
        cached_content=cache.name,
    ),
)
print(response.usage_metadata)
# ( e.g., prompt_token_count: ..., cached_content_token_count: ..., candidates_token_count: ... )
client.caches.delete(name=cache.name)count_tokens.py
```

```
  // Make sure to include the following import:
  // import {GoogleGenAI} from '@google/genai';
  const ai = new GoogleGenAI({ apiKey: process.env.GEMINI_API_KEY });
  const textFile = await ai.files.upload({
    file: path.join(media, "a11.txt"),
    config: { mimeType: "text/plain" },
  });

  const cache = await ai.caches.create({
    model: "gemini-1.5-flash-001",
    config: {
      contents: createUserContent([\
        "Here the Apollo 11 transcript:",\
        createPartFromUri(textFile.uri, textFile.mimeType),\
      ]),
      system_instruction: null,
      tools: null,
    },
  });

  const prompt = "Please give a short summary of this file.";
  const countTokensResponse = await ai.models.countTokens({
    model: "gemini-2.0-flash",
    contents: prompt,
  });
  console.log(countTokensResponse.totalTokens);

  const generateResponse = await ai.models.generateContent({
    model: "gemini-1.5-flash-001",
    contents: prompt,
    config: { cachedContent: cache.name },
  });
  console.log(generateResponse.usageMetadata);

  await ai.caches.delete({ name: cache.name });
  return {
    totalTokens: countTokensResponse.totalTokens,
    usage: generateResponse.usageMetadata,
  };
}
count_tokens.js
```

```
ctx := context.Background()
client, err := genai.NewClient(ctx, &genai.ClientConfig{
	APIKey:  os.Getenv("GEMINI_API_KEY"),
	Backend: genai.BackendGeminiAPI,
})
if err != nil {
	log.Fatal(err)
}

file, err := client.Files.UploadFromPath(
	ctx,
	filepath.Join(getMedia(), "a11.txt"),
	&genai.UploadFileConfig{
		MIMEType : "text/plain",
	},
)
if err != nil {
	log.Fatal(err)
}
parts := []*genai.Part{
	genai.NewPartFromText("Here the Apollo 11 transcript:"),
	genai.NewPartFromURI(file.URI, file.MIMEType),
}
contents := []*genai.Content{
	genai.NewContentFromParts(parts, genai.RoleUser),
}

// Create cached content using a simple slice with text and a file.
cache, err := client.Caches.Create(ctx, "gemini-1.5-flash-001", &genai.CreateCachedContentConfig{
	Contents: contents,
})
if err != nil {
	log.Fatal(err)
}

prompt := "Please give a short summary of this file."
countResp, err := client.Models.CountTokens(ctx, "gemini-2.0-flash", []*genai.Content{
	genai.NewContentFromText(prompt, genai.RoleUser),
}, nil)
if err != nil {
	log.Fatal(err)
}
fmt.Printf("%d", countResp.TotalTokens)
response, err := client.Models.GenerateContent(ctx, "gemini-1.5-flash-001", []*genai.Content{
	genai.NewContentFromText(prompt, genai.RoleUser),
}, &genai.GenerateContentConfig{
	CachedContent: cache.Name,
})
if err != nil {
	log.Fatal(err)
}

usageMetadata, err := json.MarshalIndent(response.UsageMetadata, "", "  ")
if err != nil {
	log.Fatal(err)
}
// Returns `nil` for some reason
fmt.Println(string(usageMetadata))
_, err = client.Caches.Delete(ctx, cache.Name, &genai.DeleteCachedContentConfig{})count_tokens.go
```

[Go](https://ai.google.dev/api/tokens?hl=pt-br#go)Mais

```
ctx := context.Background()
client, err := genai.NewClient(ctx, &genai.ClientConfig{
	APIKey:  os.Getenv("GEMINI_API_KEY"),
	Backend: genai.BackendGeminiAPI,
})
if err != nil {
	log.Fatal(err)
}

// Construct the user message contents.
contents := []*genai.Content{
	genai.NewContentFromText("Good morning! How are you?", genai.RoleUser),
}

// Set the system instruction as a *genai.Content.
config := &genai.GenerateContentConfig{
	SystemInstruction: genai.NewContentFromText("You are a cat. Your name is Neko.", genai.RoleUser),
}

response, err := client.Models.GenerateContent(ctx, "gemini-2.0-flash", contents, config)
if err != nil {
	log.Fatal(err)
}
printResponse(response)system_instruction.go
```

[Java](https://ai.google.dev/api/tokens?hl=pt-br#java)Mais

```
FunctionDeclaration multiplyDefinition =
    defineFunction(
        /* name  */ "multiply",
        /* description */ "returns a * b.",
        /* parameters */ Arrays.asList(
            Schema.numDouble("a", "First parameter"),
            Schema.numDouble("b", "Second parameter")),
        /* required */ Arrays.asList("a", "b"));

Tool tool = new Tool(Arrays.asList(multiplyDefinition), null);
;

// Specify a Gemini model appropriate for your use case
GenerativeModel gm =
    new GenerativeModel(
        /* modelName */ "gemini-1.5-flash",
        // Access your API key as a Build Configuration variable (see "Set up your API key"
        // above)
        /* apiKey */ BuildConfig.apiKey,
        /* generationConfig (optional) */ null,
        /* safetySettings (optional) */ null,
        /* requestOptions (optional) */ new RequestOptions(),
        /* tools (optional) */ Arrays.asList(tool));
GenerativeModelFutures model = GenerativeModelFutures.from(gm);

Content inputContent = new Content.Builder().addText("What's your name?.").build();

// For illustrative purposes only. You should use an executor that fits your needs.
Executor executor = Executors.newSingleThreadExecutor();

// For text-only input
ListenableFuture<CountTokensResponse> countTokensResponse = model.countTokens(inputContent);

Futures.addCallback(
    countTokensResponse,
    new FutureCallback<CountTokensResponse>() {
      @Override
      public void onSuccess(CountTokensResponse result) {
        int totalTokens = result.getTotalTokens();
        System.out.println("TotalTokens = " + totalTokens);
      }

      @Override
      public void onFailure(Throwable t) {
        t.printStackTrace();
      }
    },
    executor);count_tokens.java
```

### Corpo da resposta

Uma resposta de `models.countTokens`.

Ele retorna o `tokenCount` do modelo para o `prompt`.

Se bem-sucedido, o corpo da resposta incluirá dados com a estrutura a seguir:

Campos

`totalTokens``integer`

O número de tokens que o `Model` tokeniza o `prompt`. Sempre não negativo.

`cachedContentTokenCount``integer`

Número de tokens na parte em cache do comando (o conteúdo em cache).

`promptTokensDetails[]``object (ModalityTokenCount)`

Apenas saída. Lista de modalidades processadas na entrada da solicitação.

`cacheTokensDetails[]``object (ModalityTokenCount)`

Apenas saída. Lista de modalidades processadas no conteúdo em cache.

| Representação JSON |
| --- |
| ```<br>{<br>  "totalTokens": integer,<br>  "cachedContentTokenCount": integer,<br>  "promptTokensDetails": [<br>    {<br>      object (ModalityTokenCount)<br>    }<br>  ],<br>  "cacheTokensDetails": [<br>    {<br>      object (ModalityTokenCount)<br>    }<br>  ]<br>}<br>``` |

Isso foi útil?



 Envie comentários



Exceto em caso de indicação contrária, o conteúdo desta página é licenciado de acordo com a [Licença de atribuição 4.0 do Creative Commons](https://creativecommons.org/licenses/by/4.0/), e as amostras de código são licenciadas de acordo com a [Licença Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0). Para mais detalhes, consulte as [políticas do site do Google Developers](https://developers.google.com/site-policies?hl=pt-br). Java é uma marca registrada da Oracle e/ou afiliadas.

Última atualização 2025-11-20 UTC.


Quer enviar seu feedback?






\[\[\["Fácil de entender","easyToUnderstand","thumb-up"\],\["Meu problema foi resolvido","solvedMyProblem","thumb-up"\],\["Outro","otherUp","thumb-up"\]\],\[\["Não contém as informações de que eu preciso","missingTheInformationINeed","thumb-down"\],\["Muito complicado / etapas demais","tooComplicatedTooManySteps","thumb-down"\],\["Desatualizado","outOfDate","thumb-down"\],\["Problema na tradução","translationIssue","thumb-down"\],\["Problema com as amostras / o código","samplesCodeIssue","thumb-down"\],\["Outro","otherDown","thumb-down"\]\],\["Última atualização 2025-11-20 UTC."\],\[\],\[\]\]